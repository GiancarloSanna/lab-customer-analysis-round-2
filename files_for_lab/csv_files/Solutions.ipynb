{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33df2c70",
   "metadata": {},
   "source": [
    "## Round 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f39523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing basic libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59272f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the initial dataframe\n",
    "df = pd.read_csv('./marketing_customer_analysis.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81170ecd",
   "metadata": {},
   "source": [
    "1. Show the dataframe shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9352080d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56325d79",
   "metadata": {},
   "source": [
    "2. Standardize the header names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a499ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the header names\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f621ccac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the index column\n",
    "df=df.drop(['Unnamed: 0'], axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a765c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have to convert it so snake format\n",
    "cols = []\n",
    "for column in df.columns:\n",
    "    cols.append(column.lower())\n",
    "df.columns = cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fc6f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if it worked\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657b7a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace spaces with _\n",
    "cols = []\n",
    "for column in df.columns:\n",
    "    cols.append(column.replace(' ','_'))\n",
    "df.columns = cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374dd52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if it worked\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c07dffd",
   "metadata": {},
   "source": [
    "3. Which columns are numerical?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75c54c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(['int32', 'float64'])\n",
    "# df.select_dtypes(number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceaab1f",
   "metadata": {},
   "source": [
    "4. Which columns are categorical?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8978d2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(['object'])\n",
    "# df.select_dtypes(object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e3d4aa",
   "metadata": {},
   "source": [
    "5. Check and deal with NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c396c14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for NaN values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5204d2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We drop the rows where we have NaN for responses, because otherwise later we cant translate it to\n",
    "# numericals for our later analysis\n",
    "# Inserting the mode would tip the balance heavily into one direction\n",
    "df = df[df['response'].isna()==False] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaef2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6421188a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For vehicle_type we have a value A, we assume it stands for \"automatic transmission\" and fill the NAN with M for \"manual transmission\"\n",
    "df['vehicle_type'] = df['vehicle_type'].fillna('M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3eb063",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ce4d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vehicle class and vehicle size aren't known in the same rows, we cant extrapolate anything and add anew category \"unknown_size\"\"unknown_class\n",
    "df['vehicle_class'] = df['vehicle_class'].fillna('unknown_class')\n",
    "df['vehicle_size'] = df['vehicle_size'].fillna('unknown_size')\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e576d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 'number_of_open_complaints' we fill with the mode because nearly 80% have 0.0 open complaints\n",
    "df['number_of_open_complaints'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34752a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['number_of_open_complaints'] = df['number_of_open_complaints'].fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7653e3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 'months_since_last_claim' we fill with the mean\n",
    "df['months_since_last_claim'].value_counts()\n",
    "mslc= df['months_since_last_claim'].mean()\n",
    "df['months_since_last_claim'] = df['months_since_last_claim'].fillna(mslc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c02498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All NULL values are dealt with\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59828f8",
   "metadata": {},
   "source": [
    "6. Datetime format - Extract the months from the dataset and store in a separate column. Then filter the data to show only the information for the first quarter , ie. January, February and March. Hint: If data from March does not exist, consider only January and February."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a17647d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the month into a new column\n",
    "import time\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3139293b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['month'] = pd.to_datetime(df['effective_to_date'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76187efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['month'] = pd.DatetimeIndex(df['month']).month\n",
    "#df['month'] = pd.DatetimeIndex(df['month']).dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebe1c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['month'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5795c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is already just data for january and february in the source file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba5d07f",
   "metadata": {},
   "source": [
    "## Bonus put the data cleaning process into a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbf2205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ca_clean(df):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import time\n",
    "    from datetime import date\n",
    "    # drop the index column\n",
    "    df=df.drop(['Unnamed: 0'], axis=1) \n",
    "    cols = []\n",
    "    # snakecase\n",
    "    for column in df.columns:\n",
    "        cols.append(column.lower())\n",
    "    df.columns = cols\n",
    "    cols = []\n",
    "    for column in df.columns:\n",
    "        cols.append(column.replace(' ','_'))\n",
    "    df.columns = cols\n",
    "    # dropping NAN in response\n",
    "    df = df[df['response'].isna()==False]\n",
    "    # converting NAN to 'M' for vehicle type\n",
    "    df['vehicle_type'] = df['vehicle_type'].fillna('M')\n",
    "    # filling NAN for vehicle class and vehicle size\n",
    "    df['vehicle_class'] = df['vehicle_class'].fillna('unknown_class')\n",
    "    df['vehicle_size'] = df['vehicle_size'].fillna('unknown_size')\n",
    "    # For 'months_since_last_claim' we fill with the mean\n",
    "    df['months_since_last_claim'].value_counts()\n",
    "    mslc= df['months_since_last_claim'].mean()\n",
    "    df['months_since_last_claim'] = df['months_since_last_claim'].fillna(int(round(mslc)))\n",
    "    # Filling number of complaints with 0 as the most common value\n",
    "    df['number_of_open_complaints'] = df['number_of_open_complaints'].fillna(0)\n",
    "    # Getting the months from the dates\n",
    "    df['month'] = pd.to_datetime(df['effective_to_date'], errors='coerce')\n",
    "    df['month'] = pd.DatetimeIndex(df['month']).month\n",
    "    # Dropping the 'effective_to_date' columns\n",
    "    df=df.drop(['effective_to_date'], axis=1) \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61704a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying out the function:\n",
    "df2 = pd.read_csv('./marketing_customer_analysis.csv')\n",
    "df3 = ca_clean(df2)\n",
    "print(df2.shape,df3.shape)\n",
    "print(df3.columns)\n",
    "print(df3.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec0036b",
   "metadata": {},
   "source": [
    "## Round 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bab5b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Show DataFrame info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90992f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e023f6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Describe the DataFrame\n",
    "\n",
    "# For categorical data\n",
    "df.describe(include=[np.object]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b97591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For numerical data\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef6f311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the following analysis we convert the responses to a numerical value, 'Yes' = 1, 'No' = 0\n",
    "df['response_rate'] = df['response'].apply(lambda x : 1 if x == 'Yes' else 0)\n",
    "#df['response'].map({'Yes':1,'No':0}) also possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc906f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing needed libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9cedd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d64db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Show a plot of the total number of responses\n",
    "sns.countplot(x='response_rate', data=df)\n",
    "plt.ylabel('Total number of responses')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3652617f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Show a plot of the response rate by the sales channel\n",
    "sns.barplot(data=df, x='sales_channel', y='response_rate', ci = None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2779b570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 5.Show a plot of the response rate by the total claim amount\n",
    "# We get too many Bars if we directly create a plot, so we first have to\n",
    "# get a new structure into the Data, we can sort it by quantiles.\n",
    "df['quantile_claim_amount'] = pd.qcut(df['total_claim_amount'], 8, labels=False, duplicates = 'drop')\n",
    "sns.barplot(data=df, x='quantile_claim_amount', y='response_rate', estimator= np.mean, ci=None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484cc268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Show a plot of the response rate by income\n",
    "df['quantile_income'] = pd.qcut(df['income'], 12, labels=False, duplicates = 'drop')\n",
    "sns.barplot(data=df, x='quantile_income', y='response_rate', estimator= np.mean, ci=None)\n",
    "plt.show()\n",
    "# a lower number of bins shows a quite uniform distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1545b79b",
   "metadata": {},
   "source": [
    "## Round 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd978ea",
   "metadata": {},
   "source": [
    "1. Check the data types of the columns. Get the numeric data into dataframe called numerical and categorical columns in a dataframe called categoricals. (You can use np.number and np.object to select the numerical data types and categorical data types respectively)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090b57dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944a33ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical = df.select_dtypes('number')\n",
    "# for better visibility we drop some values we dont need, like 'month', 'quntile_claim_amount','quantile_income', 'response_rate'\n",
    "# we created them for analytical purposes earlier\n",
    "numerical = numerical.drop(['month', 'quantile_claim_amount','quantile_income','response_rate'], axis=1)\n",
    "numerical.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fa7cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = df.select_dtypes('object')\n",
    "categoricals.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5f13b2",
   "metadata": {},
   "source": [
    "Now we will try to check the normality of the numerical variables visually"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebd0710",
   "metadata": {},
   "source": [
    "Use seaborn library to construct distribution plots for the numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b6a269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "for i in numerical.columns:\n",
    "    sns.displot(numerical[i], bins = 100)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54edecf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in numerical.columns:\n",
    "    plt.hist(numerical[i])\n",
    "    plt.title(i)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6164b083",
   "metadata": {},
   "source": [
    "Do the distributions for different numerical variables look like a normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfa169d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few charts look like normal distributions, those that do are:\n",
    "# Income (apart from the 0 value outliers)\n",
    "# months_since_policy_inception (thogh very flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd14409",
   "metadata": {},
   "source": [
    "Drop one of the two features that show a high correlation between them (greater than 0.9). Write code for both the correlation matrix and for seaborn heatmap. If there is no pair of features that have a high correlation, then do not drop any features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ec05fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "correlations_matrix = numerical.corr()\n",
    "correlations_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94ceb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heat map\n",
    "plt.figure(figsize = (16,5))\n",
    "sns.heatmap(correlations_matrix, annot=True)\n",
    "plt.show()\n",
    "# There is nothing to drop according to the task, no correlation is anywhere near 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df20087",
   "metadata": {},
   "source": [
    "## Round 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96717b5",
   "metadata": {},
   "source": [
    "X-y splt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5a2afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting y to our target\n",
    "y = df['total_claim_amount']\n",
    "# Putting everything else to x\n",
    "X = df.drop(['total_claim_amount'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071a65b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting X into numericals and categoricals\n",
    "X_num = X.select_dtypes('number')\n",
    "X_cat = X.select_dtypes('object')\n",
    "print(X_num.columns)\n",
    "print(X_cat.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6eb20bc",
   "metadata": {},
   "source": [
    "Normalize (numerical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d66d7c",
   "metadata": {},
   "source": [
    "First we do the MinMaxScaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2df32b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea84ee5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the data in our transformer\n",
    "transformer = MinMaxScaler().fit(X_num)\n",
    "X_normalized = transformer.transform(X_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7e7c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To visualize as a dataframe\n",
    "pd.DataFrame(X_normalized, columns=X_num.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1bd7dc",
   "metadata": {},
   "source": [
    "Now the standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe909e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We again fit our data\n",
    "transformer = StandardScaler().fit(X_num)\n",
    "x_standardized2 = transformer.transform(X_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f79691",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check if it worked\n",
    "pd.DataFrame(x_standardized2, columns=X_num.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5db7bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36e6e000",
   "metadata": {},
   "source": [
    "# Round 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00c06d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We drop some columns of the categoricals because they don't deliver any value\n",
    "# 'customer' are all unique values that dont generate a pattern\n",
    "# 'response' because we have response rate in the numericals\n",
    "# 'effective_to_date' because we use the months\n",
    "X_cat = X_cat.drop(['customer','response','effective_to_date'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ba78cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For X_num we drop the quentiles we calculated, becaus ethat is already represented in the original datas\n",
    "X_num = X_num.drop(['quantile_income','quantile_claim_amount'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9ebb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normaliziong again with the dropped columns\n",
    "transformer = MinMaxScaler().fit(X_num)\n",
    "X_normalized = transformer.transform(X_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4732abb9",
   "metadata": {},
   "source": [
    "## OneHot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71688b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot/Label Encoding categoricals\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b977964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneHotEncoding of the categorical values\n",
    "encoder = OneHotEncoder(drop='first').fit(X_cat)\n",
    "print(encoder.categories_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbd44a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneHotEncoding of the categorical values\n",
    "encoded = encoder.transform(X_cat).toarray()\n",
    "encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cc79b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating the normalized and encoded data\n",
    "# Putting the encoded data into a dataframe to make it possible to concatenate\n",
    "onehot_encoded = pd.DataFrame(encoded)\n",
    "X_normalized = pd.DataFrame(X_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68d3b33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = pd.concat([X_normalized, onehot_encoded], axis=1)\n",
    "# See if it worked\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317cd6b0",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7918422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75554c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Checking the results\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa01d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Linear Regression model with our data\n",
    "from sklearn import linear_model\n",
    "lm = linear_model.LinearRegression()\n",
    "lm.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a8f7c2",
   "metadata": {},
   "source": [
    "## Model Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3277f69b",
   "metadata": {},
   "source": [
    "First we validate the performance on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c4f96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for the R2-Score\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "predictions = lm.predict(X_train)\n",
    "r2_score(y_train, predictions)\n",
    "# We achieve an R2-Score of 0.86 for our training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d368058c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for Mean Square Error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse=mean_squared_error(y_train,predictions)\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c814c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Root Mean Square Error is just the root of above\n",
    "rmse = np.sqrt(mse)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2e7514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lastly the Mean Absolute Error\n",
    "mae = mean_absolute_error(y_train, predictions)\n",
    "print(mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79be628d",
   "metadata": {},
   "source": [
    "Now we test for performance on the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794ce3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for the R2-Score\n",
    "predictions_test = lm.predict(X_test)\n",
    "r2_score(y_test, predictions_test)\n",
    "# R2S went from 0.86 to 0.82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b00a674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for Mean Square Error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse_test=mean_squared_error(y_test, predictions_test)\n",
    "mse_test\n",
    "# MSE went from about 12.000 to about 15.500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29164aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Root Mean Square Error is just the root of above\n",
    "rmse_test = np.sqrt(mse_test)\n",
    "rmse_test\n",
    "# RMSE went from about 110 to about 125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e7a0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lastly the Mean Absolute Error\n",
    "mae_test = mean_absolute_error(y_test, predictions_test)\n",
    "print(mae_test)\n",
    "# MAE went from about 65 to about 68"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2923de5",
   "metadata": {},
   "source": [
    "# Round 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ad4e70",
   "metadata": {},
   "source": [
    "To make things easier looking forward I summarize the modeling and validation process into a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d79baee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function takes (a dataframe, a target column name, a float for the test_size between 0 and 1)\n",
    "# It does:\n",
    "# X-y split\n",
    "# Num-Cat split for X\n",
    "# Normalization using MinMax\n",
    "# OneHotEncoding for Categroricals\n",
    "# Concatenation\n",
    "# Creating and training a linear regression model\n",
    "# Model Validation\n",
    "# It outputs: (liner_model, a dataframe containing the valuation metrics for the training and test data)\n",
    "\n",
    "def linear_automodel_MM(df, target, ts):\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    # X-y split\n",
    "    y = df[target]\n",
    "    X = df.drop([target], axis=1)\n",
    "    # Num-Cat split\n",
    "    X_num = X.select_dtypes('number')\n",
    "    X_cat = X.select_dtypes('object')\n",
    "    # MinMaxScaling\n",
    "    transformer = MinMaxScaler().fit(X_num)\n",
    "    X_normalized = transformer.transform(X_num)\n",
    "    \n",
    "    # OneHotEncoding\n",
    "    encoder = OneHotEncoder(drop='first').fit(X_cat)\n",
    "    encoded = encoder.transform(X_cat).toarray()\n",
    "    # Putting into dataframes\n",
    "    onehot_encoded = pd.DataFrame(encoded)\n",
    "    X_normalized = pd.DataFrame(X_normalized)\n",
    "    X = pd.concat([X_normalized, onehot_encoded], axis=1)\n",
    "    # Creating the linear regression model\n",
    "    # Train-test split\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=ts, random_state=42)\n",
    "    # Training the Linear Regression model with our data\n",
    "    from sklearn import linear_model\n",
    "    lm = linear_model.LinearRegression()\n",
    "    lm.fit(X_train,y_train)\n",
    "    \n",
    "    # Validating Model\n",
    "    # Training Data\n",
    "    # R2-Score\n",
    "    from sklearn.metrics import r2_score, mean_absolute_error\n",
    "    predictions = lm.predict(X_train)\n",
    "    r2score = r2_score(y_train, predictions)\n",
    "    \n",
    "    # Mean Square Error\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    mse=mean_squared_error(y_train,predictions)\n",
    "    \n",
    "    # Root Mean Square Error\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    # Mean Absolute Error\n",
    "    mae = mean_absolute_error(y_train, predictions)\n",
    "    \n",
    "    # Test Data\n",
    "    \n",
    "    # R2-Score\n",
    "    predictions = lm.predict(X_test)\n",
    "    r2scoret = r2_score(y_test, predictions)\n",
    "    \n",
    "    # Mean Square Error\n",
    "    mset=mean_squared_error(y_test,predictions)\n",
    "    \n",
    "    # Root Mean Square Error\n",
    "    rmset = np.sqrt(mse)\n",
    "    \n",
    "    # Mean Absolute Error\n",
    "    maet = mean_absolute_error(y_test, predictions)\n",
    "    \n",
    "    # Creating the output dataframe\n",
    "    df_val = pd.DataFrame({'ValType': ['Train', 'Test'], 'R2-Score': [r2score, r2scoret], 'MSE': [mse, mset] , 'RMSE': [rmse, rmset], 'MAE': [mae, maet]})\n",
    "    \n",
    "    # returning the model and the validation\n",
    "    return lm, df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8c7ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as above but with StandardScaling\n",
    "def linear_automodel_SS(df, target, ts):\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    # X-y split\n",
    "    y = df[target]\n",
    "    X = df.drop([target], axis=1)\n",
    "    # Num-Cat split\n",
    "    X_num = X.select_dtypes('number')\n",
    "    X_cat = X.select_dtypes('object')\n",
    "    # StandardScaling\n",
    "    transformer = StandardScaler().fit(X_num)\n",
    "    X_normalized = transformer.transform(X_num)\n",
    "    \n",
    "    # OneHotEncoding\n",
    "    encoder = OneHotEncoder(drop='first').fit(X_cat)\n",
    "    encoded = encoder.transform(X_cat).toarray()\n",
    "    # Putting into dataframes\n",
    "    onehot_encoded = pd.DataFrame(encoded)\n",
    "    X_normalized = pd.DataFrame(X_normalized)\n",
    "    X = pd.concat([X_normalized, onehot_encoded], axis=1)\n",
    "    # Creating the linear regression model\n",
    "    # Train-test split\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=ts, random_state=42)\n",
    "    # Training the Linear Regression model with our data\n",
    "    from sklearn import linear_model\n",
    "    lm = linear_model.LinearRegression()\n",
    "    lm.fit(X_train,y_train)\n",
    "    \n",
    "    # Validating Model\n",
    "    # Training Data\n",
    "    # R2-Score\n",
    "    from sklearn.metrics import r2_score, mean_absolute_error\n",
    "    predictions = lm.predict(X_train)\n",
    "    r2score = r2_score(y_train, predictions)\n",
    "    \n",
    "    # Mean Square Error\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    mse=mean_squared_error(y_train,predictions)\n",
    "    \n",
    "    # Root Mean Square Error\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    # Mean Absolute Error\n",
    "    mae = mean_absolute_error(y_train, predictions)\n",
    "    \n",
    "    # Test Data\n",
    "    \n",
    "    # R2-Score\n",
    "    predictions = lm.predict(X_test)\n",
    "    r2scoret = r2_score(y_test, predictions)\n",
    "    \n",
    "    # Mean Square Error\n",
    "    mset=mean_squared_error(y_test,predictions)\n",
    "    \n",
    "    # Root Mean Square Error\n",
    "    rmset = np.sqrt(mse)\n",
    "    \n",
    "    # Mean Absolute Error\n",
    "    maet = mean_absolute_error(y_test, predictions)\n",
    "    \n",
    "    # Creating the output dataframe\n",
    "    df_val = pd.DataFrame({'ValType': ['Train', 'Test'], 'R2-Score': [r2score, r2scoret], 'MSE': [mse, mset] , 'RMSE': [rmse, rmset], 'MAE': [mae, maet]})\n",
    "    \n",
    "    # returning the model and the validation\n",
    "    return lm, df_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed44325",
   "metadata": {},
   "source": [
    "# Actual start of round 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc402ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recreating our baseline from round 6\n",
    "df2 = df.drop(['customer','response','effective_to_date'], axis = 1)\n",
    "df2 = df2.drop(['quantile_income','quantile_claim_amount'], axis = 1)\n",
    "lm, validation = linear_automodel_MM(df2,'total_claim_amount',0.2)\n",
    "validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2d80a1",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb9005c",
   "metadata": {},
   "source": [
    "Trying out the same but StandardScaling instead of MinMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbd5b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm, validation = linear_automodel_SS(df2,'total_claim_amount',0.2)\n",
    "validation\n",
    "# Our results are a little worse, but in an insignificant matter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5fe632",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934b0e6b",
   "metadata": {},
   "source": [
    "### We try our model with different ratios of train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91249ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm, validation = linear_automodel_MM(df2,'total_claim_amount',0.1)\n",
    "validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f50f017",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm, validation = linear_automodel_MM(df2,'total_claim_amount',0.3)\n",
    "validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9a9154",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm, validation = linear_automodel_MM(df2,'total_claim_amount',0.5)\n",
    "validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d21396",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm, validation = linear_automodel_MM(df2,'total_claim_amount',0.8)\n",
    "validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c3bbe8",
   "metadata": {},
   "source": [
    "For lower training sizes we see improving performance on the training data.\n",
    "For the test data the improvement stops roughly at a 50/50 split, for our sample size this seems to be the best we can get for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbab995",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1426d355",
   "metadata": {},
   "source": [
    "### We remove some outliers in the data for income and total_claim_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb6baff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We see a large number of 0 for income, which doesn't make sense, we will remiove them\n",
    "# Since income has a noticeable correlation \"-0.35\" with our target, I decided against inserting a value into this column and\n",
    "# opted for the nuclear option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555b373f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfo = df2[(df['income']>0)].copy()\n",
    "dfo.shape\n",
    "# Check if it worked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66658af0",
   "metadata": {},
   "source": [
    "Testing our model with the new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d680930",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm, validation = linear_automodel_SS(dfo,'total_claim_amount',0.3)\n",
    "validation\n",
    "# Conclusion: Our validation metrics significantly improved, we will continue with the dfo dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d165ab94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we remove the outliers for total_claim_amount which start at above 2500\n",
    "dfo = dfo[(df['total_claim_amount']<2500)].copy()\n",
    "dfo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b5a91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm, validation = linear_automodel_SS(dfo,'total_claim_amount',0.3)\n",
    "validation\n",
    "# We only lost 3 rows but noticeably improved the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe249ef1",
   "metadata": {},
   "source": [
    "### Use the transformation on numerical columns which align it more towards a normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e1e108",
   "metadata": {},
   "source": [
    "Log10 transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c7ee7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will convert the numerical data with a log10 transformation and see the impact\n",
    "# We use this function\n",
    "def log_transfom_clean(x):\n",
    "    x = np.log10(x)\n",
    "    if np.isfinite(x):\n",
    "        return x\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f59055e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfol = dfo.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40541db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We apply the log transformastion to all numerical columns\n",
    "for c in dfol.columns:\n",
    "    if c in dfol.select_dtypes(['number']):\n",
    "        dfol[c] = dfol[c].apply(log_transfom_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165b40ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the result\n",
    "lm, validation = linear_automodel_SS(dfol,'total_claim_amount',0.2)\n",
    "validation\n",
    "# Our R2 deteriorated massively.\n",
    "# All other metrics decreased by several magnitudes. This is obviously partially due to the smaller numbers after the transformation.\n",
    "# We will consider the dfo and dfol dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff8c82c",
   "metadata": {},
   "source": [
    " ## Removing insignificant variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c0f4c0",
   "metadata": {},
   "source": [
    "We try out what effect the removal of different columns has on our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ce75b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We drop each column once and check the results\n",
    "dfod = dfo.copy()\n",
    "for c in dfod.columns:\n",
    "    if c != 'total_claim_amount':\n",
    "        dtest = dfod.copy()\n",
    "        dtest = dtest.drop([c], axis=1)\n",
    "        lm, validation = linear_automodel_SS(dtest,'total_claim_amount',0.3)\n",
    "        print('Results for dropping ' + c)\n",
    "        display(validation)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5e058e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The labels with the highest impact are:\n",
    "# 'location code'\n",
    "# 'monthly_premium_auto'\n",
    "\n",
    "# We try our model just with these columns:\n",
    "dfreduced = dfo[['location_code','monthly_premium_auto','total_claim_amount']].copy()\n",
    "lm, validationf = linear_automodel_SS(dfreduced,'total_claim_amount',0.3)\n",
    "validationf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd9de67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And with our log10 adjusted data:\n",
    "dfreduced = dfol[['location_code','monthly_premium_auto','total_claim_amount']].copy()\n",
    "lm, validationf2 = linear_automodel_SS(dfreduced,'total_claim_amount',0.3)\n",
    "validationf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc332801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With just tweo labels left there is no need to search for further multicollinearity\n",
    "# Trying out the 50/50 train test split from earlier doesn't imrpove our results anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f093c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best I can achieve are models that:\n",
    "# Take only the labels 'location_code' and 'monthly_premium_auto'\n",
    "# They require very little information and return passable results!\n",
    "# When using new data we can see if the lowered MSE RMSE and MAE from the log10 transformation outweigh the R2-Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ede66b4",
   "metadata": {},
   "source": [
    "## Bonus: Build a function for round 2 and round 7 processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c154859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use our function from round 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e6f567",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataproc(df):\n",
    "    # Cleaning the data\n",
    "    df = ca_clean(df)\n",
    "    # Removing outliers\n",
    "    df = df[(df['income']>0)]\n",
    "    df = df[(df['total_claim_amount']<2500)]\n",
    "    # Removing unneccesary columns\n",
    "    ## Keeping for eventual later use: df = df.drop(['customer','response','effective_to_date'], axis = 1)\n",
    "    df = df[['location_code','monthly_premium_auto','total_claim_amount']]\n",
    "    # log10 transformation\n",
    "    def log_transfom_clean_intern(x):\n",
    "        x = np.log10(x)\n",
    "        if np.isfinite(x):\n",
    "            return x\n",
    "        else:\n",
    "            return 0\n",
    "    for c in df.columns:\n",
    "        if c in df.select_dtypes(['number']):\n",
    "            df[c] = df[c].apply(log_transfom_clean_intern)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b223f205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying out our functions:\n",
    "testframe = pd.read_csv('./marketing_customer_analysis.csv')\n",
    "lmtest, validationtest = linear_automodel_SS(dataproc(testframe),'total_claim_amount',0.3)\n",
    "validationtest\n",
    "\n",
    "# Looks identical to our original results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b722d2d",
   "metadata": {},
   "source": [
    "# Final round"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054e990e",
   "metadata": {},
   "source": [
    "### Further data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a1dc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(numerical)\n",
    "# By visual inspection we see the same correlations as when building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ce77c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for effects in the categorical data\n",
    "for i in ['state', 'coverage', 'education', 'employmentstatus', 'gender', 'location_code',\n",
    "       'marital_status', 'policy_type', 'policy', 'renew_offer_type',\n",
    "       'sales_channel', 'vehicle_class', 'vehicle_size', 'vehicle_type']:\n",
    "    sns.barplot(data=df, x= i, y='total_claim_amount', estimator= np.mean, ci=None)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e4b7b3",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48ce570",
   "metadata": {},
   "source": [
    "### Data quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a94219",
   "metadata": {},
   "source": [
    "There seems to be a problem with the data collection for the transmission type. All data for manual transmission contained NULL values and had to be replaced.\n",
    "Also a lot of incomes are registered as 0, which does not make any sense and implies wrong entries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2706fba6",
   "metadata": {},
   "source": [
    "### Very few correlations with other numerical data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ae46a6",
   "metadata": {},
   "source": [
    "We found that in our dataset our target had very few correlations with other variables in the numerical Data, the only relevant factors seem to be the location code and the monthly premium."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b240ca02",
   "metadata": {},
   "source": [
    "When building our model the dropping of any of the categorical columns did not yield in diminishing returns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfd2f4d",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353d3dcc",
   "metadata": {},
   "source": [
    "With only two categories of our dataset we were able to produce a satisfactory model with the following metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e303812",
   "metadata": {},
   "outputs": [],
   "source": [
    "display('With normalized data:', validationf, 'With log10 transformed and normalized data:',validationf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b9a151",
   "metadata": {},
   "source": [
    "### Other factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dd692a",
   "metadata": {},
   "source": [
    "We see that the total claim amount is higher for higher coverages, what is not surprising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a28ec95",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=df, x= 'coverage', y='total_claim_amount', estimator= np.mean, ci=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2572b9",
   "metadata": {},
   "source": [
    "We see diminishing claim amounts for higher education individuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31470559",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=df, x= 'education', y='total_claim_amount', estimator= np.mean, ci=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe68026c",
   "metadata": {},
   "source": [
    "Employed customers have less claim amounts then unemployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dc516b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=df, x= 'employmentstatus', y='total_claim_amount', estimator= np.mean, ci=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb2cfd3",
   "metadata": {},
   "source": [
    "Claim amounts for male customers are slightly higher than for females."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce70431",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=df, x= 'gender', y='total_claim_amount', estimator= np.mean, ci=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d69066d",
   "metadata": {},
   "source": [
    "We see a the highest average claim for suburban customers. Rural customers are having the smallest claims.\n",
    "The differences here are huge, with Suburban averages being more than 4 times the Rural averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ed7a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=df, x= 'location_code', y='total_claim_amount', estimator= np.mean, ci=None)\n",
    "print(df['location_code'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f561c0",
   "metadata": {},
   "source": [
    "We see higher claim amounts for single customers in relation to married or divorced ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3fcb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=df, x= 'marital_status', y='total_claim_amount', estimator= np.mean, ci=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09103423",
   "metadata": {},
   "source": [
    "We see spikes in the claim amounts for luxury cars and luxury SUVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffdf132",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.barplot(data=df, x= 'vehicle_class', y='total_claim_amount', estimator= np.mean, ci=None)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac798d7",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabdfc28",
   "metadata": {},
   "source": [
    "More than 50% of the customers reside in suburban areas which tend to vastly higher claim amounts.\n",
    "\n",
    "Actions in marketing to target rural population would result in a customer base with significantly lower claims.\n",
    "This could considerably improve financial performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
